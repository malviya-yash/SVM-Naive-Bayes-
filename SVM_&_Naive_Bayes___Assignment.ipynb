{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#SVM & Naive Bayes\n",
        "Q1. What is a Support Vector Machine (SVM), and how does it work?\n",
        "\n",
        "    - A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks SVM is to find the best boundry, or \"hyperplans,\" that separates data points of different classes with the largest possible margin. SVM choosen the boundary tha maximizes the distance (margin) between the hyperplane and the closest data points from each class, which are called support vectors , SVM improve the model this optimal margin, SVM improves the model's ability to generalize to unseen data.\n",
        "\n",
        "Q2. Explain the difference between Hard Margin and Soft Margin SVM.\n",
        "\n",
        "    - A Hard margin SVM tries to seprate the groups perfectly, with no mistake-it only works if the data is easy to separate. A Soft Margin SVM allows some mistakes, which is better for real-life messy data that can't be split perfectly.\n",
        "\n",
        "Q3. What is the Kernel Trick in SVM? Give one example of a kernel and\n",
        "explain its use case.\n",
        "\n",
        "    - The Kernal Trick is a smart way SVM uses to handle data that isn't easy to split with a straigth line. it turns the data into a new shape so a straight line works. for example, the RBF (Radio Basis Function) kernal is often used for things like image recognition, where the data is not simply attanged.\n",
        "\n",
        "Q4. What is a Naïve Bayes Classifier, and why is it called “naïve”?\n",
        "\n",
        "    - Naive bayse is a way to guess what category something belongs to, based on probability. it's called \"naive\" because it assumps that all features (like the world in an email) are totally independent of each other-which is a big, and usally wrong, assumption but it often  works well anyway.\n",
        "\n",
        "Q5. Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants.\n",
        "When would you use each one?\n",
        "\n",
        "    - * Gaussian Navive Bauyes: Use when your features are numbers (like heights or weights).\n",
        "\n",
        "      * Multinomial Navive Bayes: Good for counting things, like how many times a word appears in a bunch of emails.\n",
        "\n",
        "      * Bernouil Naive Bayes: For features that are just Yes/no or 0/1, like if a word is present or not.\n",
        "\n",
        "Q10. : Imagine you’re working as a data scientist for a company that handles\n",
        "email communications.\n",
        "Your task is to automatically classify emails as Spam or Not Spam. The emails may\n",
        "contain:\n",
        "\n",
        "● Text with diverse vocabulary\n",
        "\n",
        "● Potential class imbalance (far more legitimate emails than spam)\n",
        "\n",
        "● Some incomplete or missing data\n",
        "\n",
        "Explain the approach you would take to:\n",
        "\n",
        "● Preprocess the data (e.g. text vectorization, handling missing data)\n",
        "\n",
        "● Choose and justify an appropriate model (SVM vs. Naïve Bayes)\n",
        "\n",
        "● Address class imbalance\n",
        "\n",
        "● Evaluate the performance of your solution with suitable metrics\n",
        "And explain the business impact of your solution.\n",
        "\n",
        "\n",
        "  -   * Preprocess: Turn Emails into numbers using something like TF-IDF, fill in any missing data, and clean the text.\n",
        "\n",
        "      * Pick a Model: Naive Bayes is good for this because it's fast and works well with words.\n",
        "\n",
        "      * Class Imbalance: If there's more non=spam than spam, use balancing tricks like oversampling spam emails, or give them a bigger \"weight.\"\n",
        "\n",
        "      * Evaluation: Check precision and recall (especially recall for spam-don't miss spam!) and ROC-AUC.\n",
        "\n",
        "     * Business Impact: Good spam filters block junk and save time, keep users safe, and help the company look professional.\n",
        "       "
      ],
      "metadata": {
        "id": "yobXORm8ezW9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. Write a Python program to:\n",
        "\n",
        "    * Load the Iris dataset\n",
        "\n",
        "    * Train an SVM Classifier with a linear kernel\n",
        "\n",
        "    * Print the model's accuracy and support vectors.\n"
      ],
      "metadata": {
        "id": "Vd-2CcUdtBNt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8R55q0keovd",
        "outputId": "f93adba0-69d7-45da-aff0-a0d1b871559f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Support vectors: [[4.8 3.4 1.9 0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.7 3.  5.  1.7]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [6.3 2.5 5.  1.9]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [5.9 3.  5.1 1.8]\n",
            " [4.9 2.5 4.5 1.7]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target,\n",
        "                                                    test_size=0.2, random_state=42)\n",
        "svm = SVC(kernel='linear')\n",
        "svm.fit(X_train, y_train)\n",
        "accuracy = svm.score(X_test, y_test)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Support vectors:\", svm.support_vectors_)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. Write a Python program to:\n",
        "\n",
        "● Load the Breast Cancer dataset\n",
        "\n",
        "● Train a Gaussian Naïve Bayes model\n",
        "\n",
        "● Print its classification report including precision, recall, and F1-score.\n"
      ],
      "metadata": {
        "id": "fzotp5OcvUb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target,\n",
        "                                                    test_size=0.2, random_state=42)\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "print(classification_report(y_test, model.predict(X_test)))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruYPdv0-uuZl",
        "outputId": "5ca645ca-e508-4bd8-f191-a37b8a593015"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.93      0.96        43\n",
            "           1       0.96      1.00      0.98        71\n",
            "\n",
            "    accuracy                           0.97       114\n",
            "   macro avg       0.98      0.97      0.97       114\n",
            "weighted avg       0.97      0.97      0.97       114\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.8 Write a Python program to:\n",
        "\n",
        "● Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best\n",
        "C and gamma.\n",
        "\n",
        "● Print the best hyperparameters and accuracy.\n"
      ],
      "metadata": {
        "id": "Wx0qq1FcxwXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "wine = load_wine()\n",
        "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target,\n",
        "                                                    test_size=0.2, random_state=42)\n",
        "param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [1, 0.1, 0.01, 0.001]}\n",
        "grid = GridSearchCV(SVC(), param_grid, cv=3)\n",
        "grid.fit(X_train, y_train)\n",
        "print(\"Best hyperparameters:\", grid.best_params_)\n",
        "print(\"Accuracy:\", grid.score(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqEO5lGmxP1q",
        "outputId": "eb3046f2-ac22-4902-ed0f-0bd1000d4e5f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters: {'C': 10, 'gamma': 0.001}\n",
            "Accuracy: 0.8333333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9. Write a Python program to:\n",
        "\n",
        "● Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using\n",
        "sklearn.datasets.fetch_20newsgroups).\n",
        "\n",
        "● Print the model's ROC-AUC score for its predictions.\n"
      ],
      "metadata": {
        "id": "TxxL_BWN0Yr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "categories = ['rec.sport.baseball', 'sci.space']\n",
        "data = fetch_20newsgroups(subset='train', categories=categories)\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target,\n",
        "                                                    test_size=0.2, random_state=42)\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train_vec, y_train)\n",
        "y_probs = nb.predict_proba(X_test_vec)[:, 1]\n",
        "print(\"ROC-AUC:\", roc_auc_score(y_test, y_probs))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-e7NJvL0XDV",
        "outputId": "7012dc54-aefb-4399-92c9-94bd060a9e13"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC: 0.9982905982905983\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Nv3xmfhO8Ei1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This is anwer\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ITiXghkn3ORM",
        "outputId": "53fb1921-4f12-4dbf-f31e-cb05f036795e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nThis is anwer\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YxsJPegu_Fr8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}